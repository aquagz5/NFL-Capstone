{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d1d6ba-6501-4807-88b4-8dd3ee447035",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# LSTM and GRU Implementations\n",
    "\n",
    "The following notebook is used to implement the RNN model of LSTM and GRU on our training data. We will be training the data by sequences of individual players on a given play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227390b6-8538-43b2-95b2-3cbb090f23a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Run the model prep notebok\n",
    "%run /Workspace/Repos/anthony.m.quagliata@vanderbilt.edu/NFL-Capstone/03-Models/Model_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e1da9b-817a-4573-9d03-02048cc300c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/anthony.m.quagliata@vanderbilt.edu/NFL-Capstone/03-Models/Model_Evaluation_Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813f23c2-8280-4bc4-be42-841060800561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abbf434-aa22-4c4a-acb0-7ef0c0835128",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the directory where your data is located\n",
    "directory = \"/dbfs/mnt/nfl/\"\n",
    "\n",
    "# Read in all your datasets\n",
    "games = pd.read_csv(f\"{directory}games.csv\")\n",
    "players = pd.read_csv(f\"{directory}players.csv\")\n",
    "plays = pd.read_csv(f\"{directory}plays.csv\")\n",
    "tackles = pd.read_csv(f\"{directory}tackles.csv\")\n",
    "train = pd.read_csv(f\"{directory}train_sample.csv\")\n",
    "val = pd.read_csv(f\"{directory}val_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016f1245-d32e-4e78-ae9a-489be3515f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out.view(-1,77,142)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        out = self.sigmoid(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bfb0349-4bd2-48a3-8ae3-cdb78ceda94d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_nn(train_loader, learn_rate = 0.01,batch_size=1000, hidden_dim=256, EPOCHS=5, model_type=\"GRU\", num_classes = 2):\n",
    "    \n",
    "    #Define class weights\n",
    "    class_counts = [0]*num_classes\n",
    "    \n",
    "    # Iterate through the training data to count class occurrences\n",
    "    for _, labels in train_loader:\n",
    "        for label in labels.view(-1, 142):\n",
    "            for frame_label in label:\n",
    "                class_counts[frame_label] += 1\n",
    "            \n",
    "    total_samples = sum(class_counts)\n",
    "    class_weights = [total_samples/(num_classes*count) for count in class_counts]\n",
    "    class_weights = torch.tensor(class_weights)\n",
    "    \n",
    "    print(\"Calculated training weights\")\n",
    "    print(class_weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = next(iter(train_loader))[1].shape[1]\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight = class_weights[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            out = out.squeeze()\n",
    "            labels = label.view(-1,1).to(device).float()\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    for x, label in test_loader:\n",
    "        out, _ = model(x.to(device).float(), None)\n",
    "        out = out.squeeze()\n",
    "        outputs.append(out.cpu().detach().numpy())\n",
    "        targets.append(label.numpy())\n",
    "\n",
    "    print(\"Evaluation Time: {}\".format(str(time.time()-start_time)))\n",
    "    \n",
    "    # Calculate log loss (cross-entropy loss) for classification\n",
    "    log_losses = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Calculate log loss (cross-entropy loss) for classification\n",
    "    log_losses = []\n",
    "    for i in range(len(outputs)):\n",
    "        labels = torch.from_numpy(targets[i]).float()\n",
    "        log_loss_value = criterion(torch.from_numpy(outputs[i]), labels)\n",
    "        log_losses.append(log_loss_value.item())\n",
    "\n",
    "    avg_log_loss = np.mean(log_losses)\n",
    "    print(\"Average Log Loss: {}\".format(avg_log_loss))\n",
    "    return avg_log_loss,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf4c3cf-4353-4907-bbb3-1ce658ff3941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_tensors_rnn_3d(data, target, is_synthetic = False):\n",
    "\n",
    "    #import libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    if is_synthetic:\n",
    "        #tranform gamePlayId variable to account for synthetic data\n",
    "        # Create a mask to identify duplicates based on 'gamePlayId', 'frameId', and 'nflId'\n",
    "        duplicates_mask = data.duplicated(subset=['gamePlayId', 'frameId', 'nflId'], keep='first')\n",
    "\n",
    "        # Add '.1' to 'gamePlayId' for the second occurrence of each duplicate\n",
    "        data.loc[duplicates_mask, 'gamePlayId'] += '.1'\n",
    "        \n",
    "        \n",
    "    #Preprocess data correctly\n",
    "    target_variables = [\"tackle_multiple\", \"tackle_single\"]\n",
    "\n",
    "    #determine target variables to remove\n",
    "    target_variables.remove(target)\n",
    "\n",
    "    data = data.sort_values(['gameId','playId','nflId','frameId'],ascending = [True, True, True, True])\n",
    "\n",
    "    # remove unwanted variables \n",
    "    df = data.drop([\"gameId\",\"playId\"], axis = 1)\n",
    "    df = df.drop(target_variables, axis = 1)\n",
    "\n",
    "    # Separate numerical and categorical variables\n",
    "    numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_vars = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define variables to exclude\n",
    "    exclude_scaling = ['nflId', 'frameId'] #might need to change this depending on added variables\n",
    "    exclude_scaling.append(target)\n",
    "    exclude_ohe = ['gamePlayId']\n",
    "\n",
    "    # Scale numerical variables using StandardScaler, excluding variables\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_vars.difference(exclude_scaling)] = scaler.fit_transform(df[numerical_vars.difference(exclude_scaling)])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=categorical_vars.difference(exclude_ohe), drop_first=True)\n",
    "\n",
    "\n",
    "    ############################################################################################\n",
    "    #Group data into correct array format\n",
    "\n",
    "    # Group data by 'gamePlayId'\n",
    "    plays_grouped = df.groupby('gamePlayId')\n",
    "\n",
    "    # Determine the maximum number of frames per play and rows per frame\n",
    "    max_rows_per_frame = 142 #Max rows per per frame in our data is 140, so \n",
    "    num_feature_cols = df.shape[1] - 4 #number of columns - we are droping gamePlayId, frameId, nflId, and target\n",
    "\n",
    "    #Columns to drop in loop\n",
    "    cols_to_drop = ['gamePlayId', 'nflId','frameId','tackle_multiple']\n",
    "\n",
    "    # Initialize lists for all plays' features, labels, and masks\n",
    "    all_player_features = []\n",
    "    all_player_labels = []\n",
    "    all_player_masks = []\n",
    "\n",
    "    # Initialize list to keep track of gameId, playId, nflId, frameId\n",
    "    gamePlayId_list = []\n",
    "    nflId_list = []\n",
    "    frameId_list = []\n",
    "\n",
    "    for play_id, play_data in plays_grouped:\n",
    "        # Group by 'frameId' within each play\n",
    "        players_grouped = play_data.groupby('nflId')\n",
    "\n",
    "        for player_id, player_data in players_grouped:\n",
    "            #drop grouping variables and target variable\n",
    "            features = player_data.drop(cols_to_drop, axis=1).values #remove the grouping variables\n",
    "            labels = player_data[target].values\n",
    "\n",
    "            # Extract the game, play, nflId, and frameId values for this player's data\n",
    "            gamePlayId = player_data['gamePlayId'].values[0]\n",
    "            nflId = player_data['nflId'].values[0]\n",
    "            frameId = player_data['frameId'].values[0]\n",
    "\n",
    "            # Calculate current frame length for the player\n",
    "            frame_length = len(features)\n",
    "\n",
    "            # Pad each player's features and labels to have the same number of rows\n",
    "            padded_features = np.pad(features, ((0, max_rows_per_frame - frame_length), (0, 0)), mode='constant', constant_values=0)\n",
    "            padded_labels = np.pad(labels, (0, max_rows_per_frame - frame_length), mode='constant', constant_values=0)\n",
    "\n",
    "            # Create mask for the current frame\n",
    "            mask = np.ones(max_rows_per_frame)\n",
    "            mask[:frame_length] = 1  # Actual data\n",
    "            mask[frame_length:] = 0  # Padded data\n",
    "\n",
    "            all_player_features.append(padded_features)\n",
    "            all_player_labels.append(padded_labels)\n",
    "            all_player_masks.append(mask)\n",
    "            gamePlayId_list.extend([gamePlayId] * max_rows_per_frame)\n",
    "            nflId_list.extend([nflId] * max_rows_per_frame)\n",
    "            frameId_list.extend([frameId + i for i in range(max_rows_per_frame)])\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    x = np.array(all_player_features, dtype=np.float32)\n",
    "    y = np.array(all_player_labels, dtype=np.int64)\n",
    "    mask_array = np.array(all_player_masks, dtype=np.int64)\n",
    "    id_data = pd.DataFrame({'gamePlayId': gamePlayId_list, 'nflId': nflId_list, 'frameId': frameId_list})\n",
    "\n",
    "    return torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(mask_array), id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2181e2ab-96d6-4231-814f-12e35406de16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7330835-3509-4ec1-a7bb-d6de0dfd7271",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba24c1f1-db11-4e1e-a2f5-fedaeeed0a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x, y, mask, id_data = data_tensors_rnn_3d(train, \"tackle_single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9a7c03-c79c-446b-8677-372e4de18621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7ce34d-a6ed-4745-bf9d-725c0e869894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 77\n",
    "train_data = TensorDataset(x,y)\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6b9d8d-0939-4dc1-ba5f-a0dc72aeba0b",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gru_model = train_nn(train_loader, batch_size = batch_size, EPOCHS = 3, model_type=\"GRU\")\n",
    "#Lstm_model = train_nn(train_loader, batch_size = batch_size, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b9d939-d46e-4043-b7d6-001e9906db9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_log_loss_gru,outputs_gru = evaluate(gru_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb59a337-0f2e-4cc5-9afc-502aa87edf01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example flattened mask as a tensor\n",
    "flattened_mask = mask.view(-1)\n",
    "# Create a flattened mask as a list of True and False values\n",
    "flattened_mask = (flattened_mask == 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718bcd11-2fa4-4259-bcf0-ba029d1be81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Flattening those GRU values to get outputs with right values\n",
    "flattened_values_gru = np.concatenate(outputs_gru).ravel()\n",
    "# Create a new DataFrame from the flattened array\n",
    "df_flattened_gru = pd.DataFrame(flattened_values_gru, columns=['model_probs_GRU'])\n",
    "#concatenate the probs with the id data\n",
    "pred_df_gru = pd.concat([id_data, df_flattened_gru], axis=1)\n",
    "pred_df_gru = pred_df_gru[flattened_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63edfefb-bdaa-4294-9e5a-0c4de125fe55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pred_df_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f84bb8-bb23-46aa-a4fa-3a50a3e22e62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LSTM and GRU Implementation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
